{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a.\n",
    "\n",
    "An abstract superclass in object oriented programming facilitates modularity by defining the built-in subclasses in order to swap out pieces of information without affecting the code. As a result, it allows for the usage of different parts of more than one machiene learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For non-object-oriented languages that use data structures and functions on their own, the modular components require clear defintions for each function documentation-wise so that people know how each module connects. For example, there should be definitions for the purpose of each function and how different functions can work together, the required inputs, and the outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinReg:\n",
    "        #Train the linear regression model\n",
    "        #X is the input data\n",
    "        #y is the target values (vector)\n",
    "        #hparameters: hyperparameters for training.\n",
    "    \n",
    "    #this is how we store the different things we need to store by using the self. functions \n",
    "    def __init__(self):\n",
    "        self.doesithaveparameters = False  # to see if it has it been trained or not\n",
    "        self.parameters = None  # has the parameters\n",
    "        self.hparameters = {}  # has hyperparameters\n",
    "    \n",
    "    def LinRegtrain(self, X, y, hparameters):\n",
    "        #this stores the hyperparameters\n",
    "        self.hparameters = hparameters\n",
    "        # now add the training logic\n",
    "        #to find the parameters in linear regression the equation is: \n",
    "        #theta=(((transposed X)*X)^-1)*((transposed X)*(y output values))\n",
    "        #The X matrix (input) is transposed (T)\n",
    "        #np.linalg.pinv() is a numpy function that does the pseudoinverse\n",
    "        #of the matrix so that it can work with matracies that are different \n",
    "        #shapes instead of focusing on square matracies it is useful for least\n",
    "        #square solutions. Also is able to work with matracies that can not be inversed. \n",
    "        #then this is multiplied the transposed X matrix and then by y (output or target values)\n",
    "        self.parameters = np.linalg.pinv(X.T @ X) @ X.T @ y\n",
    "        #changed to true once training sucessfully done \n",
    "        self.doesithaveparameters = True\n",
    "\n",
    "    def LinRegpredict(self, X):\n",
    "        #Now we use the parameters that have been learned in order to predict\n",
    "        #X is the input data\n",
    "        #if there are no parameters identified, then an error will be raised in order to train the model\n",
    "        if not self.doesithaveparameters:\n",
    "            raise ValueError(\"Train the model before predicting.\")\n",
    "        # Compute the predictions using the learned parameters and input data\n",
    "        #this np.dot() is a numpy function that is able to multiply X (input data) that are\n",
    "        #1 (vectors, dot product) or 2 (matracie multiplication) dimensional\n",
    "        #so, X which holds the the matrix size and self.parameters hold the weight\n",
    "        #of the features are multiplied in order to find the predicted values\n",
    "        #based on the parameters  \n",
    "        #as a result, the putput is the predicted values for the inputs in vactor/one dimension\n",
    "        return np.dot(X,self.parameters) \n",
    "\n",
    "    #returns the parameters\n",
    "    def linreggetparameters(self):\n",
    "        return self.parameters\n",
    "    \n",
    "    #returns hyperparameters\n",
    "    def linreggethparameters(self):\n",
    "        return self.hparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogReg:\n",
    "    \n",
    "    #this is how we store the different things we need to store by using the self. functions \n",
    "    def __init__(self):\n",
    "        self.doesithaveparameters = False  #to see if it has it been trained or not\n",
    "        self.parameters = None  #has the parameters\n",
    "        self.hparameters = {}  #has hyperparameters\n",
    "    \n",
    "    def LogRegtrain(self, X, y, hparameters):\n",
    "        #Train the logistic regression model.\n",
    "        #X is the input data\n",
    "        #y is the target values (vector)\n",
    "        #hparameters: hyperparameters for training.\n",
    "\n",
    "        #lrate is responsible for the size of each step for each loop\n",
    "        #amount of times while running the loop, smaller the number the \n",
    "        #more it is gradual and more accurate. if the number is bigger \n",
    "        #then it is faster but might not show optimal solution. \n",
    "        #hparameters.get() stores the rate obtained in dictionary\n",
    "        lrate = hparameters.get('lrate', 0.01)\n",
    "        #looop is responsible for the amt of times it loops in training\n",
    "        #updates parameters \n",
    "        #the more loops the more it can adijst its fit \n",
    "        #stores amt of loops in hparameters dictionary \n",
    "        looop = hparameters.get('looop', 1000)\n",
    "        #stores weight of model of corresponting input data\n",
    "        #numpy function (np.zeros) that makes array for amt of features in X\n",
    "        self.parameters = np.zeros(X.shape[1])\n",
    "\n",
    "        #now we are going to use a gradient descent loop\n",
    "        #tells for loop to keep going for the value of looop\n",
    "        for _ in range(looop):\n",
    "            #variable for predictions \n",
    "            #self._sigmoid is a function used in order to make values 0-1 \n",
    "            #X @ self.parameters does matrix multiplication of input \n",
    "            #data and parameters\n",
    "            predi = self.sigmoidd(X @ self.parameters)\n",
    "            #finds the change of actual vs predicted \n",
    "            #multiply transposed X and the change calcualted \n",
    "            #then it is divided by the # of samples to get averages \n",
    "            #this showcases how it is changing so that we can\n",
    "            #identify how to reduce loss\n",
    "            gradi = X.T @ (predi - y) / len(y)\n",
    "            #updating the parameters by subtracting to see how it compares\n",
    "            self.parameters -= lrate * gradi\n",
    "        #changed to true once training sucessfully done \n",
    "        self.doesithaveparameters = True\n",
    "\n",
    "    def LogRegpredict(self, X):\n",
    "        #Now we use the parameters that have been learned in order to predict\n",
    "        #X is the input data\n",
    "        #if there are no parameters identified, then an error will be raised in order to train the model\n",
    "        if not self.doesithaveparameters:\n",
    "            raise ValueError(\"Train the model before predicting.\")\n",
    "        #multiply X by parameters and then values are converted to either 0 or 1\n",
    "        #if >= 0.5 then it is 1 and if less then it is 0 \n",
    "        #then boolean is make a integer \n",
    "        return (self.sigmoidd(X @ self.parameters) >= 0.5).astype(int)\n",
    "\n",
    "    def sigmoidd(self, z):\n",
    "        #uses z (input which is X multiplies by parameters) to \n",
    "        #get a number 1-0 to see if prediction is close enough to actual\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "        #returns the parameters\n",
    "    def logreggetparameters(self):\n",
    "        return self.parameters\n",
    "    \n",
    "    #returns hyperparameters\n",
    "    def logreggethparameters(self):\n",
    "        return self.hparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These variables are able to recieve their assignments by using the parameters, hparameters, and doesithaveparameters. The parameters allows for the identification of the parameters when the model goes into training and from the learning process of the data. hparameters is obtained from the training step and the input. doesithaveparameters changes from false to true once it passes through the training stage. This allows for the model to identify that it is now ready to produce predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method is preferred because it allows for the prevention of unwanted changes, showcases consistancy through the different updates to the model, produces simpler models, and decreases the probability of error by avoiding manual work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For hyperparameters a hashmap or dictionary is is more appropriate. Hyperparameters use names and keys for different values within different data types in order to be understandable and efficient\n",
    "\n",
    "For parameters, an array or matrix implementation is more appropriate. Parameters are integers, and operations are used with them. Making it easier to compute calculations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinReg:\n",
    "    def __init__(self):\n",
    "        self.doesithaveparameters = False\n",
    "        self.parameters = None\n",
    "        self.hparameters = {}\n",
    "        #this definition is responsible for the training of the 3 \n",
    "        #variations of the lin reg models \n",
    "    def LinRegtrain(self, X, y, hparameters):\n",
    "        self.hparameters = hparameters\n",
    "        #hparameters is a dictionary and it stores info on the variation, sigma, b2 and lambda\n",
    "        #to make the data easier to work with it is normalized using mean and standard deviaiton. \n",
    "        #This is done to y and X \n",
    "        self.meanofy = np.mean(y)\n",
    "        self.standardizedy = np.std(y)\n",
    "        y = (y - self.meanofy) / self.standardizedy\n",
    "        self.meanofx = np.mean(X, axis=0)\n",
    "        self.standardizedx = np.std(X, axis=0)\n",
    "        #the 1e-8 is used in order to prevent illegal mathematical calculations like dividing by 0\n",
    "        X = (X - self.meanofx) / (self.standardizedx + 1e-8)\n",
    "        #this creates a column of 1s for bias which helps when making predictiosn \n",
    "        X = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "        #if it is not defined, it will process as MLE \n",
    "        version = hparameters.get('version', 'MLE')\n",
    "        print(f\"starting linear regression training with {version}\")\n",
    "\n",
    "        #this helps identify which version to train the model with\n",
    "        #for map, we use sigma and b2 which is for varience of noise and of the prior \n",
    "        if version == 'MAP':\n",
    "            sig = hparameters.get('sig', 1.0)\n",
    "            b2 = hparameters.get('b2', 1.0)\n",
    "            self.MAPtraining(X, y, sig, b2)\n",
    "        #lambda helps with overfitting \n",
    "        elif version == 'regularization':\n",
    "            lambd = hparameters.get('lambd', 1.0)\n",
    "            self.REGtraining(X, y, lambd)\n",
    "        #basic lin reg with nothing else \n",
    "        elif version == 'MLE':\n",
    "            self.MLEtraining(X, y)\n",
    "        #if there are parameters then we can conclude training. \n",
    "        self.doesithaveparameters = True\n",
    "        print(\"all variations of linear regression training are done.\")\n",
    "    #explained in previous question \n",
    "    def MLEtraining(self, X, y):\n",
    "        self.parameters = np.linalg.pinv(X.T @ X) @ X.T @ y\n",
    "    #helps with regularization\n",
    "    #adds negative reward for big numbers prefers smaller numbers  \n",
    "    def MAPtraining(self, X, y, sig, b2):\n",
    "        XT = X.T\n",
    "        M = XT @ X + (sig / b2) * np.identity(XT.shape[0])\n",
    "        inverseM = np.linalg.pinv(M)\n",
    "        self.parameters = inverseM @ XT @ y\n",
    "    #this prevents overfitting by making bigger numbers smaller \n",
    "    def REGtraining(self, X, y, lambd):\n",
    "        XT = X.T\n",
    "        M = XT @ X + lambd * np.identity(XT.shape[0])\n",
    "        self.parameters = np.linalg.pinv(M) @ XT @ y\n",
    "    #if there are no parameters it tells u to train to get parameteters \n",
    "    #bias term is added and it is all 1s again \n",
    "    #uses the obtained parameters in order to initiate prediction and the inverse is calculated\n",
    "    def LinRegpredict(self, X):\n",
    "        if not self.doesithaveparameters:\n",
    "            raise ValueError(\"the model needs to be trained before it can start predicting.\")\n",
    "        X = (X - self.meanofx) / (self.standardizedx + 1e-8)\n",
    "        X = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "        predi = np.dot(X, self.parameters)\n",
    "        return predi * self.standardizedy + self.meanofy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogReg:\n",
    "    def __init__(self):\n",
    "        self.doesithaveparameters = False\n",
    "        self.parameters = None\n",
    "        self.hparameters = {}\n",
    "    #defining the learing rate, tau, and number of loops \n",
    "    def LogRegtrain(self, X, y, hparameters):\n",
    "        self.hparameters = hparameters\n",
    "        lrate = hparameters.get('lrate', 0.001)\n",
    "        tau = hparameters.get('tau', 1e-6)\n",
    "        looop = hparameters.get('looop', 1000)\n",
    "        print(f\"gradient descent training with learning rate={lrate}, tau={tau}, loops={looop}\")\n",
    "        #this part is responsible for normaliizng the data \n",
    "        #using mean and std and 1e-8 again\n",
    "        #also a column of 1s for bias \n",
    "        self.meanofx = np.mean(X, axis=0)\n",
    "        self.standardizedx = np.std(X, axis=0)\n",
    "        X = (X - self.meanofx) / (self.standardizedx + 1e-8)\n",
    "        X = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "        #the weight starts off as 0s\n",
    "        #using function to get parameters using gradient descent \n",
    "        self.parameters = np.zeros(X.shape[1])\n",
    "        self.parameters = self.graddescent(X, y, self.parameters, lrate, tau, looop)\n",
    "        #if it has parameters then training is done \n",
    "        self.doesithaveparameters = True\n",
    "        print(\"logistic regression training done.\")\n",
    "    #for loop keeps going for looop times and makes copies of past parameters \n",
    "    #sigmoid is used to predict the liklihood of each parameter \n",
    "    #gradi idenftifies the cost byt getting the difference and gets average of transposed X\n",
    "    #then parameters are updated by subtracting learning * rate gradient decent from parametser \n",
    "    #if normalized number lower than tau then it is done and parameters returned \n",
    "    def graddescent(self, X, y, parameters, lrate, tau, looop):\n",
    "        for iteration in range(looop):\n",
    "            prevpar = parameters.copy()\n",
    "            predi = self.sigimoid(X @ parameters)\n",
    "            gradi = X.T @ (predi - y) / len(y)\n",
    "            parameters -= lrate * gradi\n",
    "            if np.linalg.norm(parameters - prevpar) < tau:\n",
    "                print(f\"joining reached after {iteration + 1} iterations.\")\n",
    "                break\n",
    "        return parameters\n",
    "    #if there are no parameters then it has to be trained before prediction \n",
    "    def LogRegpredict(self, X):\n",
    "        if not self.doesithaveparameters:\n",
    "            raise ValueError(\"the model needs to be trained before it can start predicting..\")\n",
    "        #normalize the x \n",
    "        #make column of 1s \n",
    "        #sigmoid function used to get the liklihood of going to 1\n",
    "        #if probability is bigger or = to 0.5 then it goes to one if it is lower then it goes to 0 \n",
    "        X = (X - self.meanofx) / (self.standardizedx + 1e-8)\n",
    "        X = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "        probab = self.sigimoid(X @ self.parameters)\n",
    "        return (probab >= 0.5).astype(int)\n",
    "    #1/1+e^z = z makes inputs values between 0-1 \n",
    "    def sigimoid(self, z):\n",
    "        z = np.clip(z, -100, 100)\n",
    "        return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_squared_error, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class LinReg:\n",
    "    def __init__(self):\n",
    "        self.doesithaveparameters = False\n",
    "        self.parameters = None\n",
    "        self.hparameters = {}\n",
    "\n",
    "    def LinRegtrain(self, X, y, hparameters):\n",
    "        self.hparameters = hparameters\n",
    "        self.meanofy = np.mean(y)\n",
    "        self.standardizedy = np.std(y)\n",
    "        y = (y - self.meanofy) / self.standardizedy\n",
    "\n",
    "        self.meanofx = np.mean(X, axis=0)\n",
    "        self.standardizedx = np.std(X, axis=0)\n",
    "        X = (X - self.meanofx) / (self.standardizedx + 1e-8)\n",
    "\n",
    "        X = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "        version = hparameters.get('version', 'MLE')\n",
    "        print(f\"Starting linear regression training with {version}\")\n",
    "\n",
    "        if version == 'MAP':\n",
    "            sig = hparameters.get('sig', 1.0)\n",
    "            b2 = hparameters.get('b2', 1.0)\n",
    "            self.MAPtraining(X, y, sig, b2)\n",
    "\n",
    "        elif version == 'regularization':\n",
    "            lambd = hparameters.get('lambd', 1.0)\n",
    "            self.REGtraining(X, y, lambd)\n",
    "\n",
    "        elif version == 'MLE':\n",
    "            self.MLEtraining(X, y)\n",
    "\n",
    "        self.doesithaveparameters = True\n",
    "        print(\"All variations of linear regression training are done.\")\n",
    "\n",
    "    def MLEtraining(self, X, y):\n",
    "        self.parameters = np.linalg.pinv(X.T @ X) @ X.T @ y\n",
    "\n",
    "    def MAPtraining(self, X, y, sig, b2):\n",
    "        XT = X.T\n",
    "        M = XT @ X + (sig / b2) * np.identity(XT.shape[0])\n",
    "        inverseM = np.linalg.pinv(M)\n",
    "        self.parameters = inverseM @ XT @ y\n",
    "\n",
    "    def REGtraining(self, X, y, lambd):\n",
    "        XT = X.T\n",
    "        M = XT @ X + lambd * np.identity(XT.shape[0])\n",
    "        self.parameters = np.linalg.pinv(M) @ XT @ y\n",
    "\n",
    "    def LinRegpredict(self, X):\n",
    "        if not self.doesithaveparameters:\n",
    "            raise ValueError(\"The model needs to be trained before it can start predicting.\")\n",
    "        X = (X - self.meanofx) / (self.standardizedx + 1e-8)\n",
    "        X = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "        predi = np.dot(X, self.parameters)\n",
    "        return predi * self.standardizedy + self.meanofy\n",
    "\n",
    "class LogReg:\n",
    "    def __init__(self):\n",
    "        self.doesithaveparameters = False\n",
    "        self.parameters = None\n",
    "        self.hparameters = {}\n",
    "\n",
    "    def LogRegtrain(self, X, y, hparameters):\n",
    "        self.hparameters = hparameters\n",
    "        lrate = hparameters.get('lrate', 0.001)\n",
    "        tau = hparameters.get('tau', 1e-6)\n",
    "        looop = hparameters.get('looop', 1000)\n",
    "        print(f\"Gradient descent training with learning rate={lrate}, tau={tau}, loops={looop}\")\n",
    "\n",
    "        self.meanofx = np.mean(X, axis=0)\n",
    "        self.standardizedx = np.std(X, axis=0)\n",
    "        X = (X - self.meanofx) / (self.standardizedx + 1e-8)\n",
    "        X = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "        self.parameters = np.zeros(X.shape[1])\n",
    "        self.parameters = self.graddescent(X, y, self.parameters, lrate, tau, looop)\n",
    "        self.doesithaveparameters = True\n",
    "        print(\"Logistic regression gradient descent training done.\")\n",
    "\n",
    "    def graddescent(self, X, y, parameters, lrate, tau, looop):\n",
    "        for iteration in range(looop):\n",
    "            prevpar = parameters.copy()\n",
    "            predi = self.sigimoid(X @ parameters)\n",
    "            gradi = X.T @ (predi - y) / len(y)\n",
    "            parameters -= lrate * gradi\n",
    "            if np.linalg.norm(parameters - prevpar) < tau:\n",
    "                print(f\"Convergence reached after {iteration + 1} iterations.\")\n",
    "                break\n",
    "        return parameters\n",
    "\n",
    "    def LogRegpredict(self, X):\n",
    "        if not self.doesithaveparameters:\n",
    "            raise ValueError(\"Model needs to be trained before predicting.\")\n",
    "        X = (X - self.meanofx) / (self.standardizedx + 1e-8)\n",
    "        X = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "        probab = self.sigimoid(X @ self.parameters)\n",
    "        return (probab >= 0.5).astype(int)\n",
    "\n",
    "    def sigimoid(self, z):\n",
    "        z = np.clip(z, -100, 100)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "#this was made in google colab so to uplaod the dataset it might be different  \n",
    "\n",
    "#based on dataset info we find the column names. \n",
    "#feature picking specific columns to run is essential \n",
    "#makes all values numeric and removes data that is NA \n",
    "def importthedata(pathofdata, certainfeatures=None):\n",
    "    columnnames = [\n",
    "        'MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population',\n",
    "        'AveOccup', 'Latitude', 'Longitude', 'MedHouseVal'\n",
    "    ]\n",
    "    data = pd.read_csv(pathofdata, header=None, names=columnnames, on_bad_lines='skip')\n",
    "    for col in columnnames:\n",
    "        data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "    data.dropna(inplace=True)\n",
    "    #pick which features to use \n",
    "    if certainfeatures:\n",
    "        X = data[certainfeatures].values\n",
    "    #if there is no specified ones use all features \n",
    "    else:\n",
    "        X = data.iloc[:, :-1].values\n",
    "    y = data['MedHouseVal'].values\n",
    "    return X, y\n",
    "#makes training and testing data based on split \n",
    "#if it is 1.0 it goes to 0.9 so it can still test a little of it \n",
    "def partingdata(X, y, trainingsize=0.8):\n",
    "    if trainingsize == 1.0:\n",
    "        trainingsize = 0.9\n",
    "    trainn = int(len(X) * trainingsize)\n",
    "    return X[:trainn], y[:trainn], X[trainn:], y[trainn:]\n",
    "#median returned as threshold \n",
    "def willingthres(y):\n",
    "    return np.median(y)\n",
    "#this counts the amount of 0 and 1 s \n",
    "def count01(predictions, threshold):\n",
    "    binarypred = (predictions >= threshold).astype(int)\n",
    "    countof0and1 = np.bincount(binarypred)\n",
    "    countof0s = countof0and1[0] if len(countof0and1) > 0 else 0\n",
    "    countof1s = countof0and1[1] if len(countof0and1) > 1 else 0\n",
    "    return countof0s, countof1s\n",
    "#plots for accuracy precision f1 and recall \n",
    "def plottingmet(trainingsizeS, accu, preci, callback, scoredf1):\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(trainingsizeS, accu, marker='o', color='blue')\n",
    "    plt.title('Accuracy vs Training Size')\n",
    "    plt.xlabel('Percentage of Training Data')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(trainingsizeS, preci, marker='o', color='green')\n",
    "    plt.title('Precision vs Training Size')\n",
    "    plt.xlabel('Percentage of Training Data')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(trainingsizeS, callback, marker='o', color='orange')\n",
    "    plt.title('Recall vs Training Size')\n",
    "    plt.xlabel('Percentage of Training Data')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(trainingsizeS, scoredf1, marker='o', color='red')\n",
    "    plt.title('F1 score vs Training Size')\n",
    "    plt.xlabel('Percentage of Training Data')\n",
    "    plt.ylabel('F1 score')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "#RMSE plot of MAP MRE and regularization \n",
    "def RMSEplots(trainingsizeS, MLErmsee, MAPrmsee, REGrmsee):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(trainingsizeS, MLErmsee, marker='o', label='MLE', color='pink')\n",
    "    plt.plot(trainingsizeS, MAPrmsee, marker='o', label='MAP', color='purple')\n",
    "    plt.plot(trainingsizeS, REGrmsee, marker='o', label='Regularization', color='red')\n",
    "    plt.xlabel('Percentage of Training Data')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.title('RMSE Comparison for MLE, MAP and Regularization')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "#PLOTs of confusion matracies \n",
    "def confumatplot(cms, titles):\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    for idx, (cm, title) in enumerate(zip(cms, titles)):\n",
    "        row, col = divmod(idx, 3)\n",
    "        ax = axes[row, col]\n",
    "        im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "        ax.set_title(title)\n",
    "        fig.colorbar(im, ax=ax)\n",
    "        ticks = np.arange(2)\n",
    "        ax.set_xticks(ticks)\n",
    "        ax.set_xticklabels(['Predicted 0', 'Predicted 1'])\n",
    "        ax.set_yticks(ticks)\n",
    "        ax.set_yticklabels(['Actual 0', 'Actual 1'])\n",
    "        labels = np.array([[\"TN\", \"FP\"], [\"FN\", \"TP\"]])\n",
    "        thresh = cm.max() / 2\n",
    "        for i in range(cm.shape[0]):\n",
    "            for j in range(cm.shape[1]):\n",
    "                ax.text(j, i, f'{labels[i, j]}: {cm[i, j]}', horizontalalignment=\"center\",\n",
    "                        color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        ax.set_ylabel('True label')\n",
    "        ax.set_xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#main function so it can know what to run \n",
    "if __name__ == \"__main__\":\n",
    "    pathofdata = \"/content/cal_housing.data\"\n",
    "    #can run different features if u change the code with these different options and run it below this section\n",
    "    alloffeatures = ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n",
    "    basedondemo = ['MedInc', 'Population', 'AveOccup']\n",
    "    basedonhouse = ['HouseAge', 'AveRooms', 'AveBedrms']\n",
    "    basedonloc = ['Latitude', 'Longitude', 'MedInc']\n",
    "    randomcombo = ['MedInc', 'HouseAge', 'AveRooms'] \n",
    "    #pick which one u want to run and change it below\n",
    "    certainfeatures = basedondemo \n",
    "    #now update the data uploaded with the features u pcik \n",
    "    X, y = importthedata(pathofdata, certainfeatures=certainfeatures)\n",
    "    #define training sizes \n",
    "    trainingsizeS = [0.1, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "    #emplty places to store data \n",
    "    MLErmsee, MAPrmsee, REGrmsee = [], [], []\n",
    "    accu, preci, callback, scoredf1 = [], [], [], []\n",
    "    linreg = LinReg()\n",
    "    cms, titles = [], []\n",
    "#printing model info \n",
    "    for size in trainingsizeS:\n",
    "        trainingX, trainingy, testingX, testingy = partingdata(X, y, trainingsize=size)\n",
    "        print(f\"\\ntraining size: {size * 100}% of data\")\n",
    "\n",
    "        #for mle\n",
    "        linreg.LinRegtrain(trainingX, trainingy, {\"version\": \"MLE\"})\n",
    "        ypredictionofMLE = linreg.LinRegpredict(testingX)\n",
    "        MLErmse = np.sqrt(mean_squared_error(testingy, ypredictionofMLE))\n",
    "        MLErmsee.append(MLErmse)\n",
    "        print(f\"MLE RMSE: {MLErmse:,.2f}\")\n",
    "        thres = willingthres(testingy)\n",
    "        MLE0s, MLE1s = count01(ypredictionofMLE, thres)\n",
    "        print(f\"MLE model predicted counts: 0s={MLE0s}, 1s={MLE1s}\")\n",
    "\n",
    "        #for MAP\n",
    "        linreg.LinRegtrain(trainingX, trainingy, {\"version\": \"MAP\", \"sig\": 1.0, \"b2\": 1.0})\n",
    "        mapypred = linreg.LinRegpredict(testingX)\n",
    "        mappingrmse = np.sqrt(mean_squared_error(testingy, mapypred))\n",
    "        MAPrmsee.append(mappingrmse)\n",
    "        print(f\"MAP RMSE: {mappingrmse:,.2f}\")\n",
    "        MAP0s, MAP1s = count01(mapypred, thres)\n",
    "        print(f\"MAP model predicted counts: 0s={MAP0s}, 1s={MAP1s}\")\n",
    "\n",
    "        #for regularization\n",
    "        linreg.LinRegtrain(trainingX, trainingy, {\"version\": \"regularization\", \"lambda\": 1.0})\n",
    "        regypred = linreg.LinRegpredict(testingX)\n",
    "        reggingrmse = np.sqrt(mean_squared_error(testingy, regypred))\n",
    "        REGrmsee.append(reggingrmse)\n",
    "        print(f\"Regularization RMSE: {reggingrmse:,.2f}\")\n",
    "        REG0s, REG1s = count01(regypred, thres)\n",
    "        print(f\"Regularization model predicted counts: 0s={REG0s}, 1s={REG1s}\")\n",
    "\n",
    "        #willing to purchase for linear regression with function using threshold that we got from before \n",
    "        #makes 0 1 list (1yes 0no) if u can purchase \n",
    "        totalpredictiony = linreg.LinRegpredict(X)\n",
    "        thresline = willingthres(y)\n",
    "        canupurchase = (totalpredictiony >= thresline).astype(int)\n",
    "\n",
    "        #parts data basedo n canupurchase in log reg \n",
    "        #trains with hyperparameters \n",
    "        #then makes predictions \n",
    "        trainingXlog, trainingylog, testingXlog, testingylog = partingdata(X, canupurchase, trainingsize=0.8)\n",
    "        logreg = LogReg()\n",
    "        hparamslogreg = {\"lrate\": 0.001, \"tau\": 1e-6, \"looop\": 1000}\n",
    "        logreg.LogRegtrain(trainingXlog, trainingylog, hparamslogreg)\n",
    "        logregypred = logreg.LogRegpredict(testingXlog)\n",
    "\n",
    "        #uses functions to analyze log reg mdel \n",
    "        accuu = accuracy_score(testingylog, logregypred)\n",
    "        precii = precision_score(testingylog, logregypred)\n",
    "        callbackk = recall_score(testingylog, logregypred)\n",
    "        f1 = f1_score(testingylog, logregypred)\n",
    "        accu.append(accuu)\n",
    "        preci.append(precii)\n",
    "        callback.append(callbackk)\n",
    "        scoredf1.append(f1)\n",
    "\n",
    "        #confusion mattric to plot \n",
    "        cm = confusion_matrix(testingylog, logregypred)\n",
    "        cms.append(cm)\n",
    "        titles.append(f'{int(size * 100)}% Training Data')\n",
    "\n",
    "    #plot RMSE\n",
    "    RMSEplots([s * 100 for s in trainingsizeS], MLErmsee, MAPrmsee, REGrmsee)\n",
    "\n",
    "    #plot log reg measures \n",
    "    plottingmet([s * 100 for s in trainingsizeS], accu, preci, callback, scoredf1)\n",
    "\n",
    "    #plot confusion matracies \n",
    "    confumatplot(cms, titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfiting happens when the noise of the data overpowers the data itself. This can also be due to very large datasets. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "If the ground truth is added, then the model gets the answer its supposed to predict. The predictions will be 'spot on' but the model would not have really done its job and wont be able to be used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FLubsy believes simplicity over direct programming of approximations. Mathematical operations like absolute value function at the sharp point or a jump in a piecewise function, or  vertical asymptotes have the liklihood to cintain undefined gradients. As a result, unreliable predictions come from failed finite differences. If epsilon is very small then it will be harder to spot the variation which decreases the precision. This results in underflow. Also, this makes gradient results inaccurate. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
